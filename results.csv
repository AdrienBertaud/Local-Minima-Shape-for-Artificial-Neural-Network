optimizer,lr,batch size,num iteration,duration,train loss,train accuracy,test loss,test accuracy,sharpness train,non uniformity train,sharpness test,non uniformity test
sgd,0.001,10,2050,0,0.00096,100.0,1.50893,81.05000305,32,4817,189,865
adagrad,0.001,10,1850,0,0.00098,100.0,1.18753,82.90000153,13,94,391,2780
sgd,0.01,10,250,0,0.0007,100.0,1.40029,82.5,7,82,151,1402
adam,0.01,1000,400,44,2e-05,100.0,1.16743,90.15000153,0,1,61,904
adagrad,0.01,1000,400,40,0.00813,99.90000153,0.43613,91.29999542,14,111,34,301
adam,0.1,100,999,533,2.29976,11.5,6.06676,10.80000019,0,0,-3,197
adagrad,0.1,100,999,223,0.00913,99.90000153,0.96865,88.40000153,156,1356,789,5183
sgd,0.1,100,240,41,0.00264,100.0,0.53996,90.75,7,58,39,327
adagrad,0.001,1000,3200,314,0.00256,100.0,0.5453,90.25,18,102,129,946
adagrad,0.001,1000,2800,290,0.00267,100.0,0.53478,90.5999984741211,18,100,119,873
adam,0.001,1000,400,44,0.00019,100.0,0.8368399999999999,90.8499984741211,2,11,103,1012
gd,0.001,1000,9999,947,0.8458200000000001,70.09999847412111,0.8662,68.34999847412111,50,101,57,106
adagrad,0.1,1000,9999,1050,0.06858,98.1,3.42843,73.1,11,68,0,0
sgd,0.1,10,99,110,0.00345,100.0,1.19608,82.6,2,25,0,0
adagrad,0.1,10,99,167,0.00116,100.0,1.74302,81.0,1,14,0,0
